{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variants, lineages, and mutations set creation\n",
    "Variants, lineages, and mutations are of interest but are not necessarily confined to any specific topicCategory. Classifying for these might help to search for VOCs and VOIs.\n",
    "\n",
    "User regex to find specific lineages, or mutations\n",
    "\n",
    "Regex cheatcode:\n",
    "* '\\b' indicates word boundary\n",
    "* '()' indicates a capture group\n",
    "* '?:' indicates a non-capture group\n",
    "* '\\d' indicates a digit\n",
    "* '{1,5}' indicates repeat in pattern between 1 and 5 times (so in mutation example, one to five digits)\n",
    "* \"r' \" indicates raw string notation\n",
    "* (?i)(?-i) indicates everything between is case insensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = 'data/'\n",
    "RESULTSPATH = 'results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_ids_from_json(jsonfile):\n",
    "    idlist = []\n",
    "    for eachhit in jsonfile[\"hits\"]:\n",
    "        if eachhit[\"_id\"] not in idlist:\n",
    "            idlist.append(eachhit[\"_id\"])\n",
    "    return(idlist)\n",
    "\n",
    "\n",
    "\n",
    "#### Get the size of the source (to make it easy to figure out when to stop scrolling)\n",
    "def fetch_src_size(source):\n",
    "    pubmeta = requests.get(\"https://api.outbreak.info/resources/query?q=((@type:Publication) AND (curatedBy.name:\"+source+\"))&size=0&aggs=@type\")\n",
    "    pubjson = json.loads(pubmeta.text)\n",
    "    pubcount = int(pubjson[\"facets\"][\"@type\"][\"total\"])\n",
    "    return(pubcount)\n",
    "\n",
    "\n",
    "#### Ping the API and get all the ids for a specific source and scroll through the source until number of ids matches meta\n",
    "def get_source_ids(source):\n",
    "    source_size = fetch_src_size(source)\n",
    "    r = requests.get(\"https://api.outbreak.info/resources/query?q=((@type:Publication) AND (curatedBy.name:\"+source+\"))&fields=_id&fetch_all=true\")\n",
    "    response = json.loads(r.text)\n",
    "    idlist = get_ids_from_json(response)\n",
    "    try:\n",
    "        scroll_id = response[\"_scroll_id\"]\n",
    "        while len(idlist) < source_size:\n",
    "            r2 = requests.get(\"https://api.outbreak.info/resources/query?q=((@type:Publication) AND (curatedBy.name:\"+source+\"))&fields=_id&fetch_all=true&scroll_id=\"+scroll_id)\n",
    "            response2 = json.loads(r2.text)\n",
    "            idlist2 = set(get_ids_from_json(response2))\n",
    "            tmpset = set(idlist)\n",
    "            idlist = tmpset.union(idlist2)\n",
    "            try:\n",
    "                scroll_id = response2[\"_scroll_id\"]\n",
    "            except:\n",
    "                print(\"no new scroll id\")\n",
    "        return(idlist)\n",
    "    except:\n",
    "        return(idlist)\n",
    "\n",
    "\n",
    "def get_pub_ids(sourceset):\n",
    "    pub_srcs = {\"preprint\":[\"bioRxiv\",\"medRxiv\"],\"litcovid\":[\"litcovid\"],\n",
    "                \"other\":[\"Figshare\",\"Zenodo\",\"MRC Centre for Global Infectious Disease Analysis\"],\n",
    "                \"all\":[\"Figshare\",\"Zenodo\",\"MRC Centre for Global Infectious Disease Analysis\",\n",
    "                       \"bioRxiv\",\"medRxiv\",\"litcovid\"]}\n",
    "    sourcelist = pub_srcs[sourceset]\n",
    "    allids = []\n",
    "    for eachsource in sourcelist:\n",
    "        sourceids = get_source_ids(eachsource)\n",
    "        allids = list(set(allids).union(set(sourceids)))\n",
    "    return(allids)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starttime = datetime.now()\n",
    "allids = get_pub_ids('all')\n",
    "print('fetched all ids: ',datetime.now()-starttime)\n",
    "starttime = datetime.now()\n",
    "metadf = batch_fetch_dated_meta(allids)\n",
    "print('fetched all metadata: ',datetime.now()-starttime)\n",
    "starttime = datetime.now()\n",
    "textdf = dirty_merge_texts(metadf)\n",
    "print('merged all text: ',datetime.now()-starttime)\n",
    "print(textdf.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textdf.to_csv('data/textdf.txt',sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textdf = read_csv('data/textdf.txt',delimiter='\\t',header=0,index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do\n",
    "1. Find and extract mutations\n",
    "2. Check frequency of mutations in publications to see if trends can be identified\n",
    "3. Train algorithms to see if any new true positives can be identified\n",
    "3. Do the same for lineages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Mutation training set creation\n",
    "import re\n",
    "\n",
    "genes = \"(?:ORF1a|ORF1b|S|ORF3a|ORF3b|E|M|ORF6|ORF7a|ORF7b|ORF8|N|ORF9b|ORF10|ORF14|3'UTR|3UTR)\"\n",
    "#proteins = \"S|E|N|M|(NSP|Nsp(?:1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16))\"\n",
    "geneprots = r\"\\b(?:ORF1a|ORF1b|Spike|spike|ORF3a|ORF3b|Envelope|envelope|M protein|M\\(pro\\)|ORF6|ORF7a|ORF7b|ORF8|ORF9b|ORF10|ORF14|3'UTR|3UTR|(?:(?:NSP|nsp|Nsp|N)(?:1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16)))\\b\"\n",
    "mutations = \"((?:A|C|D|E|F|G|H|I|K|L|M|N|P|Q|R|S|T|V|W|Y)\\d{1,5}(?:A|C|D|E|F|G|H|I|K|L|M|N|P|Q|R|S|T|V|W|Y))\"\n",
    "#mutations = \"^(?:A|C|D|E|F|G|H|I|K|L|M|N|P|Q|R|S|T|V|W|Y)\\d{1,5}(?:A|C|D|E|F|G|H|I|K|L|M|N|P|Q|R|S|T|V|W|Y)$\"\n",
    "\n",
    "#deletions = genes+\":\"+\"(?:DEL|Del|del)\"+\"\\d{}\"\n",
    "#deletion_ex = \"ORF8∆381, ORF7a∆81 and spike∆15, ∆15, 60/70-deletion\"\n",
    "#deletion_variant = r\"\\b(∆\\d{1,5} variant)\\b\"\n",
    "\n",
    "token_dict = {\n",
    "    'mutants':r'\\b((?:A|C|D|E|F|G|H|I|K|L|M|N|P|Q|R|S|T|V|W|Y)\\d{2,5}(?:A|C|D|E|F|G|H|I|K|L|M|N|P|Q|R|S|T|V|W|Y))\\b',\n",
    "    #'genemute':r\"\\b((?:ORF1a|ORF1b|S|ORF3a|ORF3b|E|M|ORF6|ORF7a|ORF7b|ORF8|N|ORF9b|ORF10|ORF14|3'UTR|3UTR):(?:A|C|D|E|F|G|H|I|K|L|M|N|P|Q|R|S|T|V|W|Y)\\d{1,5}(?:A|C|D|E|F|G|H|I|K|L|M|N|P|Q|R|S|T|V|W|Y))\\b\",\n",
    "    'genemute':r\"\\b((?:ORF1a|ORF1b|S|Spike|spike|ORF3a|ORF3b|E|Envelope|envelope|M|M protein|M\\(pro\\)|ORF6|ORF7a|ORF7b|ORF8|ORF9b|ORF10|ORF14|3'UTR|3UTR|(?:(?:NSP|nsp|Nsp|N)(?:1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16)))(?:\\s|:)(?:A|C|D|E|F|G|H|I|K|L|M|N|P|Q|R|S|T|V|W|Y)\\d{1,5}(?:A|C|D|E|F|G|H|I|K|L|M|N|P|Q|R|S|T|V|W|Y))\\b\",\n",
    "    'deletions':r\"\\b((?:ORF1a|ORF1b|S|Spike|spike|ORF3a|ORF3b|E|Envelope|envelope|M|M protein|M\\(pro\\)|ORF6|ORF7a|ORF7b|ORF8|ORF9b|ORF10|ORF14|3'UTR|3UTR|(?:(?:NSP|nsp|Nsp|N)(?:1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16)))(?:∆|(?:DEL|Del|del|:DEL|:Del|:del))\\d{1,5})\\b\",\n",
    "    'nonspec_deletion':r\"\\b((?:ORF1a|ORF1b|S|Spike|spike|ORF3a|ORF3b|E|Envelope|envelope|M|M protein|M\\(pro\\)|ORF6|ORF7a|ORF7b|ORF8|N|ORF9b|ORF10|ORF14|3'UTR|3UTR|(?:NSP|Nsp|nsp)(?:1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16))\\s(?:(?:\\d{1,5})(?:\\/)(?:\\d{1,5})|(?:\\d{1,5}))(?:\\s|-)(?:deletion))\\b\"\n",
    "}\n",
    "\n",
    "testtext = \"This gene is full of ORF1a∆123 variants or other kinds of NSP deletion variants. There are also various mutations like S:E484K which is also known just as E484K which is a spike gene mutation. There are also deletions on the spike protein such as S:DEL15 or S∆15 though it could also be S:del15 so to speak or spike:del15 or Sdel15. Imagine if there was an ORF1a:A645T or E:C163G mutation. There are also nonspecific deletions like 60/70-deletion or 145/162-deletion or ∆15. There can also be mutations or deletions in the NSP genes or proteins such as Nsp2∆115 or NSP1:del115. Is it b.1.91 going to work or will any sort of lineageb.1.91 work nope and the spike 60/70-deletion or maybe the NSP2 459 deletion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mutationslist = pd.DataFrame(columns=['_id','name','abstract','description','text','date','mutations'])\n",
    "for eachkey in token_dict.keys():\n",
    "    tmpdf = textdf.loc[textdf['text'].str.contains(token_dict[eachkey])].copy()\n",
    "    tmpdf['mutations'] = tmpdf['text'].str.findall(token_dict[eachkey])\n",
    "    tmpmutationslist = tmpdf.explode('mutations').copy()\n",
    "    mutationslist = pd.concat((mutationslist,tmpmutationslist),ignore_index=True)\n",
    "mutationslist['date'] = pd.to_datetime(mutationslist['date'])\n",
    "mutationslist.drop_duplicates(keep='first',inplace=True)\n",
    "mutationslist['gene_mentions'] = mutationslist['text'].str.findall(geneprots)\n",
    "mutationslist['gene_mentions'] = mutationslist['gene_mentions'].apply(lambda x: lowerlist(x))\n",
    "print(mutationslist.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutationsclean = mutationslist[['_id','name','date','mutations','gene_mentions']].copy()\n",
    "mutationsclean.to_csv(os.path.join(RESULTSPATH,'mutations.tsv'),sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "humefactors = mutationslist.loc[mutationslist['text'].str.contains(\"polymorphism\")].copy()\n",
    "humefactors.drop_duplicates(keep=\"first\",inplace=True)\n",
    "humefactors.drop(columns=['abstract','text','description'],inplace=True)\n",
    "humefactors.to_csv(os.path.join(RESULTSPATH,'polymorphisms.tsv'),sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(textdf))\n",
    "#mutdf = textdf.loc[(textdf['text'].str.contains(token_dict['mutants']))].copy()\n",
    "#mutdf['mutations'] = mutdf['text'].str.findall(token_dict['mutants'])\n",
    "\n",
    "#mutdf['date'] = pd.to_datetime(mutdf['date'])\n",
    "#mutationslist = mutdf.explode('mutations').copy()\n",
    "#mutationslist.drop(['abstract','name','description'],axis=1,inplace=True)\n",
    "#sortedmutations = mutationslist.sort_values(['mutations','date'],ascending=[True,True])\n",
    "#mutationfrequency = mutationslist.groupby('mutations').size().reset_index(name='counts')\n",
    "mutationfrequency = mutationslist.groupby('mutations').resample('W-Mon', on='date').size().reset_index(name='counts').sort_values(by='date')\n",
    "mutationfrequency.sort_values(['date','counts'],ascending=[False,False],inplace=True)\n",
    "print(mutationslist.head(n=5))\n",
    "print(mutationfrequency.head(n=20))\n",
    "#singlementions = mutationfrequency['mutations'].loc[mutationfrequency['counts']<2].unique().tolist()\n",
    "#mutations2check = mutationslist.loc[mutationslist['mutations'].isin(singlementions)]\n",
    "#mutations2check.to_csv('data/variants/mutations_to_check.tsv',sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#genemutdf = textdf.loc[textdf['text'].str.contains(token_dict['genemute'],case=False)].copy()\n",
    "#genemutdf = textdf.loc[textdf['text'].str.extract(token_dict['genemute'])].copy()\n",
    "genemutdf = textdf.loc[textdf['text'].str.contains('E484K')].copy()\n",
    "print(genemutdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Variant training set creation\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Variant of Concern 202012/01\", \"VOC-202012/01\", \"20B/501Y.V1\", \"20I/501Y.V1\", the 501Y.V2 variant\n",
    "Variant Under Investigation 202012/01 (VUI 202012/01 for short)\n",
    "\"the {location} variant\"\n",
    "\"variant in {location}\"\n",
    "\"\"\"\n",
    "\n",
    "the_variant = r\"((?i)the (?:\\w*|\\w*.\\w*) variant(?-i))\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Lineage training set creation\n",
    "## More lineage names: \"https://www.the-scientist.com/news-opinion/a-guide-to-emerging-sars-cov-2-variants-68387\"\n",
    "\n",
    "## fetch lineages from :\"https://cov-lineages.org/lineages.html\"\n",
    "lineagetable = read_csv(\"https://raw.githubusercontent.com/cov-lineages/pango-designation/master/lineages.csv\",error_bad_lines=False,header=0)\n",
    "lineages = lineagetable['lineage'].loc[lineagetable['lineage'].str.len()>2].unique().tolist()\n",
    "print(lineagetable,len(lineages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikivariants = variant_names(querylist)\n",
    "wikidict = {}\n",
    "i=0\n",
    "while i < len(wikivariants):\n",
    "    wikidict[wikivariants.iloc[i]['alias'].lower().strip()] = wikivariants.iloc[i]['name'].lower().strip()\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lineage search, no additional regex formatting\n",
    "The issue with this method is that the (.) are not handled properly so things like N95 end up being included and an additional filtering is needed which may end up cutting out relevant entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterlist = list(set(lineages).union(set(wikivariants['alias'].tolist())))\n",
    "searchterm = ' | '.join(masterlist)\n",
    "filterterms = '|'.join(filter_terms)\n",
    "tmpdf = textdf.loc[textdf['text'].str.contains(searchterm)].copy()\n",
    "tmpdf['lineages'] = tmpdf['text'].str.findall(searchterm)\n",
    "rawlineageslist = tmpdf.explode('lineages').copy()\n",
    "cleanlineageslist = rawlineageslist.loc[rawlineageslist['text'].str.contains(filterterms)].copy()\n",
    "cleanlineageslist['lineages'] = [x.strip() for x in cleanlineageslist['lineages']]\n",
    "\n",
    "print(len(rawlineageslist))\n",
    "print(len(cleanlineageslist))\n",
    "    \n",
    "cleanlineageslist['lineages'].replace(wikidict,inplace=True)\n",
    "cleanlineageslist.drop_duplicates(keep='first',inplace=True)\n",
    "print(len(cleanlineageslist))\n",
    "print(cleanlineageslist.head(n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineagecounts = cleanlineageslist.groupby('lineages').size().reset_index(name='publication counts')\n",
    "lineagecounts.sort_values('publication counts',ascending=False,inplace=True)\n",
    "print(lineagecounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional regex attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterlist = list(set(lineages).union(set(wikivariants['alias'].tolist())))\n",
    "#print(len(masterlist))\n",
    "searchterm = ' | '.join(masterlist)\n",
    "#re_str1 = r'\\b(?i)('\n",
    "#re_str2 = r')(?-i)'\n",
    "#rawstring = r\"{}\".format(searchterm)\n",
    "#searchregex = re.compile(re_str1 + rawstring + re_str2)\n",
    "filterterms = '|'.join(filter_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterlist = list(set(lineages).union(set(wikivariants['alias'].tolist())))\n",
    "regexlist = []\n",
    "for eachitem in masterlist:\n",
    "    #rawstring = r\"{}\".format(eachitem.strip().replace('.','\\.'))\n",
    "    searchstring = rf\"{re.escape(eachitem)}\"\n",
    "    #searchregex = re.compile(searchstring)\n",
    "    #print(searchregex)\n",
    "    regexlist.append(searchstring)\n",
    "    #checkdf = textdf.loc[textdf['text'].str.contains(searchregex)].copy()\n",
    "    #if len(checkdf)>0:\n",
    "    #    checkdf['lineages'] = checkdf['text'].str.findall(searchregex)\n",
    "    #    print(checkdf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#searchterms = '|'.join(regexlist)\n",
    "searchregex = re.compile('|'.join(regexlist), re.IGNORECASE)\n",
    "lineagedf = textdf.loc[textdf['text'].str.contains(searchregex)].copy()\n",
    "lineagedf['lineages'] = lineagedf['text'].str.findall(searchregex)\n",
    "print(len(lineagedf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawlineageslist = lineagedf.explode('lineages').copy()\n",
    "#cleanlineageslist = rawlineageslist.loc[rawlineageslist['text'].str.contains(filterterms)].copy()\n",
    "cleanlineageslist = rawlineageslist[['_id','name','date','lineages']].copy()\n",
    "cleanlineageslist['lineages'] = [x.strip().lower() for x in cleanlineageslist['lineages']]\n",
    "\n",
    "print(len(rawlineageslist))\n",
    "print(len(cleanlineageslist))\n",
    "    \n",
    "cleanlineageslist['lineages'].replace(wikidict,inplace=True)\n",
    "cleanlineageslist.drop_duplicates(keep='first',inplace=True)\n",
    "print(len(cleanlineageslist))\n",
    "print(cleanlineageslist.head(n=10))\n",
    "cleanlineageslist.to_csv('results/lineages.tsv',sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = cleanlineageslist.groupby('lineages').size().reset_index(name='counts')\n",
    "frequency.sort_values('counts',ascending=False,inplace=True)\n",
    "print(frequency.head(n=10))\n",
    "print(frequency.tail(n=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check use of elasticsearch to correctly query for ambiguous lineages\n",
    "A development API has been created to test the effect of ignoring '.' in the indexing of name, abstract, keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_dev_size(source):\n",
    "    pubmeta = requests.get(\"https://dev.outbreak.info/resources/query?q=((@type:Publication) AND (curatedBy.name:\"+source+\"))&size=0&aggs=@type\")\n",
    "    pubjson = json.loads(pubmeta.text)\n",
    "    pubcount = int(pubjson[\"facets\"][\"@type\"][\"total\"])\n",
    "    return(pubcount)\n",
    "\n",
    "\n",
    "#### Ping the API and get all the ids for a specific source and scroll through the source until number of ids matches meta\n",
    "def get_dev_ids(source):\n",
    "    source_size = fetch_src_size(source)\n",
    "    r = requests.get(\"https://dev.outbreak.info/resources/query?q=((@type:Publication) AND (curatedBy.name:\"+source+\"))&fields=_id&fetch_all=true\")\n",
    "    response = json.loads(r.text)\n",
    "    idlist = get_ids_from_json(response)\n",
    "    try:\n",
    "        scroll_id = response[\"_scroll_id\"]\n",
    "        while len(idlist) < source_size:\n",
    "            r2 = requests.get(\"https://dev.outbreak.info/resources/query?q=((@type:Publication) AND (curatedBy.name:\"+source+\"))&fields=_id&fetch_all=true&scroll_id=\"+scroll_id)\n",
    "            response2 = json.loads(r2.text)\n",
    "            idlist2 = set(get_ids_from_json(response2))\n",
    "            tmpset = set(idlist)\n",
    "            idlist = tmpset.union(idlist2)\n",
    "            try:\n",
    "                scroll_id = response2[\"_scroll_id\"]\n",
    "            except:\n",
    "                print(\"no new scroll id\")\n",
    "        return(idlist)\n",
    "    except:\n",
    "        return(idlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Extract mutations\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from datetime import datetime\n",
    "from src.common import *\n",
    "from src.extract_variants import *\n",
    "\n",
    "DATAPATH = 'data/'\n",
    "RESULTSPATH = 'results/'\n",
    "\n",
    "\n",
    "allids = get_pub_ids('all')\n",
    "metadf = batch_fetch_meta(allids)\n",
    "textdf = merge_texts(metadf)\n",
    "mutationsclean = extract_mutations(RESULTSPATH,textdf, token_dict, export=True)\n",
    "\n",
    "## Run time of entire script: 31 min\n",
    "## Run time of just the mutation extraction: 24.7 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Extract variants\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pandas import read_csv\n",
    "from datetime import datetime\n",
    "from src.common import *\n",
    "from src.extract_variants import *\n",
    "\n",
    "DATAPATH = 'data/'\n",
    "RESULTSPATH = 'results/'\n",
    "\n",
    "#allids = get_pub_ids('all')\n",
    "#metadf = batch_fetch_meta(allids)\n",
    "#textdf = merge_texts(metadf)\n",
    "cleanlineageslist = extract_lineages(DATAPATH,RESULTSPATH,lineagequerylist,textdf,export=True)\n",
    "\n",
    "\n",
    "## Runtime: \n",
    "## Runtime of just the lineage extraction: 1 hr 5 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "textdf = read_csv('data/textdf.txt',index_col=0,header=0,delimiter='\\t')\n",
    "print(textdf.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gtsueng\\Anaconda3\\envs\\outbreak\\lib\\site-packages\\pandas\\core\\strings.py:1954: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  return func(self, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 42min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from datetime import datetime\n",
    "from src.common import *\n",
    "from src.extract_variants import *\n",
    "\n",
    "#script_path = pathlib.Path(__file__).parent.absolute()\n",
    "script_path = ''\n",
    "DATAPATH = os.path.join(script_path,'data/')\n",
    "RESULTSPATH = os.path.join(script_path,'results/')\n",
    "\n",
    "allids = get_pub_ids('all')\n",
    "metadf = batch_fetch_dated_meta(allids)\n",
    "textdf = merge_texts(metadf)\n",
    "mutationsclean = extract_mutations(RESULTSPATH, textdf, token_dict, export=True)\n",
    "cleanlineageslist = extract_lineages(DATAPATH, RESULTSPATH, lineagequerylist, textdf, export=True)\n",
    "\n",
    "## Run time 1h 42 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
