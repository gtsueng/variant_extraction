{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variants, lineages, and mutations set creation\n",
    "Variants, lineages, and mutations are of interest but are not necessarily confined to any specific topicCategory. Classifying for these might help to search for VOCs and VOIs.\n",
    "\n",
    "User regex to find specific lineages, or mutations\n",
    "\n",
    "Regex cheatcode:\n",
    "* '\\b' indicates word boundary\n",
    "* '()' indicates a capture group\n",
    "* '?:' indicates a non-capture group\n",
    "* '\\d' indicates a digit\n",
    "* '{1,5}' indicates repeat in pattern between 1 and 5 times (so in mutation example, one to five digits)\n",
    "* \"r' \" indicates raw string notation\n",
    "* (?i)(?-i) indicates everything between is case insensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = 'data/'\n",
    "RESULTSPATH = 'results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_ids_from_json(jsonfile):\n",
    "    idlist = []\n",
    "    for eachhit in jsonfile[\"hits\"]:\n",
    "        if eachhit[\"_id\"] not in idlist:\n",
    "            idlist.append(eachhit[\"_id\"])\n",
    "    return(idlist)\n",
    "\n",
    "\n",
    "\n",
    "#### Get the size of the source (to make it easy to figure out when to stop scrolling)\n",
    "def fetch_src_size(source):\n",
    "    pubmeta = requests.get(\"https://api.outbreak.info/resources/query?q=((@type:Publication) AND (curatedBy.name:\"+source+\"))&size=0&aggs=@type\")\n",
    "    pubjson = json.loads(pubmeta.text)\n",
    "    pubcount = int(pubjson[\"facets\"][\"@type\"][\"total\"])\n",
    "    return(pubcount)\n",
    "\n",
    "\n",
    "#### Ping the API and get all the ids for a specific source and scroll through the source until number of ids matches meta\n",
    "def get_source_ids(source):\n",
    "    source_size = fetch_src_size(source)\n",
    "    r = requests.get(\"https://api.outbreak.info/resources/query?q=((@type:Publication) AND (curatedBy.name:\"+source+\"))&fields=_id&fetch_all=true\")\n",
    "    response = json.loads(r.text)\n",
    "    idlist = get_ids_from_json(response)\n",
    "    try:\n",
    "        scroll_id = response[\"_scroll_id\"]\n",
    "        while len(idlist) < source_size:\n",
    "            r2 = requests.get(\"https://api.outbreak.info/resources/query?q=((@type:Publication) AND (curatedBy.name:\"+source+\"))&fields=_id&fetch_all=true&scroll_id=\"+scroll_id)\n",
    "            response2 = json.loads(r2.text)\n",
    "            idlist2 = set(get_ids_from_json(response2))\n",
    "            tmpset = set(idlist)\n",
    "            idlist = tmpset.union(idlist2)\n",
    "            try:\n",
    "                scroll_id = response2[\"_scroll_id\"]\n",
    "            except:\n",
    "                print(\"no new scroll id\")\n",
    "        return(idlist)\n",
    "    except:\n",
    "        return(idlist)\n",
    "\n",
    "\n",
    "def get_pub_ids(sourceset):\n",
    "    pub_srcs = {\"preprint\":[\"bioRxiv\",\"medRxiv\"],\"litcovid\":[\"litcovid\"],\n",
    "                \"other\":[\"Figshare\",\"Zenodo\",\"MRC Centre for Global Infectious Disease Analysis\"],\n",
    "                \"all\":[\"Figshare\",\"Zenodo\",\"MRC Centre for Global Infectious Disease Analysis\",\n",
    "                       \"bioRxiv\",\"medRxiv\",\"litcovid\"]}\n",
    "    sourcelist = pub_srcs[sourceset]\n",
    "    allids = []\n",
    "    for eachsource in sourcelist:\n",
    "        sourceids = get_source_ids(eachsource)\n",
    "        allids = list(set(allids).union(set(sourceids)))\n",
    "    return(allids)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starttime = datetime.now()\n",
    "allids = get_pub_ids('all')\n",
    "print('fetched all ids: ',datetime.now()-starttime)\n",
    "starttime = datetime.now()\n",
    "metadf = batch_fetch_dated_meta(allids)\n",
    "print('fetched all metadata: ',datetime.now()-starttime)\n",
    "starttime = datetime.now()\n",
    "textdf = dirty_merge_texts(metadf)\n",
    "print('merged all text: ',datetime.now()-starttime)\n",
    "print(textdf.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textdf.to_csv('data/textdf.txt',sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textdf = read_csv('data/textdf.txt',delimiter='\\t',header=0,index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do\n",
    "1. Find and extract mutations\n",
    "2. Check frequency of mutations in publications to see if trends can be identified\n",
    "3. Train algorithms to see if any new true positives can be identified\n",
    "3. Do the same for lineages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Mutation training set creation\n",
    "import re\n",
    "\n",
    "genes = \"(?:ORF1a|ORF1b|S|ORF3a|ORF3b|E|M|ORF6|ORF7a|ORF7b|ORF8|N|ORF9b|ORF10|ORF14|3'UTR|3UTR)\"\n",
    "#proteins = \"S|E|N|M|(NSP|Nsp(?:1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16))\"\n",
    "geneprots = r\"\\b(?:ORF1a|ORF1b|Spike|spike|ORF3a|ORF3b|Envelope|envelope|M protein|M\\(pro\\)|ORF6|ORF7a|ORF7b|ORF8|ORF9b|ORF10|ORF14|3'UTR|3UTR|(?:(?:NSP|nsp|Nsp|N)(?:1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16)))\\b\"\n",
    "mutations = \"((?:A|C|D|E|F|G|H|I|K|L|M|N|P|Q|R|S|T|V|W|Y)\\d{1,5}(?:A|C|D|E|F|G|H|I|K|L|M|N|P|Q|R|S|T|V|W|Y))\"\n",
    "#mutations = \"^(?:A|C|D|E|F|G|H|I|K|L|M|N|P|Q|R|S|T|V|W|Y)\\d{1,5}(?:A|C|D|E|F|G|H|I|K|L|M|N|P|Q|R|S|T|V|W|Y)$\"\n",
    "\n",
    "#deletions = genes+\":\"+\"(?:DEL|Del|del)\"+\"\\d{}\"\n",
    "#deletion_ex = \"ORF8∆381, ORF7a∆81 and spike∆15, ∆15, 60/70-deletion\"\n",
    "#deletion_variant = r\"\\b(∆\\d{1,5} variant)\\b\"\n",
    "\n",
    "token_dict = {\n",
    "    'mutants':r'\\b((?:A|C|D|E|F|G|H|I|K|L|M|N|P|Q|R|S|T|V|W|Y)\\d{2,5}(?:A|C|D|E|F|G|H|I|K|L|M|N|P|Q|R|S|T|V|W|Y))\\b',\n",
    "    #'genemute':r\"\\b((?:ORF1a|ORF1b|S|ORF3a|ORF3b|E|M|ORF6|ORF7a|ORF7b|ORF8|N|ORF9b|ORF10|ORF14|3'UTR|3UTR):(?:A|C|D|E|F|G|H|I|K|L|M|N|P|Q|R|S|T|V|W|Y)\\d{1,5}(?:A|C|D|E|F|G|H|I|K|L|M|N|P|Q|R|S|T|V|W|Y))\\b\",\n",
    "    'genemute':r\"\\b((?:ORF1a|ORF1b|S|Spike|spike|ORF3a|ORF3b|E|Envelope|envelope|M|M protein|M\\(pro\\)|ORF6|ORF7a|ORF7b|ORF8|ORF9b|ORF10|ORF14|3'UTR|3UTR|(?:(?:NSP|nsp|Nsp|N)(?:1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16)))(?:\\s|:)(?:A|C|D|E|F|G|H|I|K|L|M|N|P|Q|R|S|T|V|W|Y)\\d{1,5}(?:A|C|D|E|F|G|H|I|K|L|M|N|P|Q|R|S|T|V|W|Y))\\b\",\n",
    "    'deletions':r\"\\b((?:ORF1a|ORF1b|S|Spike|spike|ORF3a|ORF3b|E|Envelope|envelope|M|M protein|M\\(pro\\)|ORF6|ORF7a|ORF7b|ORF8|ORF9b|ORF10|ORF14|3'UTR|3UTR|(?:(?:NSP|nsp|Nsp|N)(?:1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16)))(?:∆|(?:DEL|Del|del|:DEL|:Del|:del))\\d{1,5})\\b\",\n",
    "    'nonspec_deletion':r\"\\b((?:ORF1a|ORF1b|S|Spike|spike|ORF3a|ORF3b|E|Envelope|envelope|M|M protein|M\\(pro\\)|ORF6|ORF7a|ORF7b|ORF8|N|ORF9b|ORF10|ORF14|3'UTR|3UTR|(?:NSP|Nsp|nsp)(?:1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16))\\s(?:(?:\\d{1,5})(?:\\/)(?:\\d{1,5})|(?:\\d{1,5}))(?:\\s|-)(?:deletion))\\b\"\n",
    "}\n",
    "\n",
    "testtext = \"This gene is full of ORF1a∆123 variants or other kinds of NSP deletion variants. There are also various mutations like S:E484K which is also known just as E484K which is a spike gene mutation. There are also deletions on the spike protein such as S:DEL15 or S∆15 though it could also be S:del15 so to speak or spike:del15 or Sdel15. Imagine if there was an ORF1a:A645T or E:C163G mutation. There are also nonspecific deletions like 60/70-deletion or 145/162-deletion or ∆15. There can also be mutations or deletions in the NSP genes or proteins such as Nsp2∆115 or NSP1:del115. Is it b.1.91 going to work or will any sort of lineageb.1.91 work nope and the spike 60/70-deletion or maybe the NSP2 459 deletion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mutationslist = pd.DataFrame(columns=['_id','name','abstract','description','text','date','mutations'])\n",
    "for eachkey in token_dict.keys():\n",
    "    tmpdf = textdf.loc[textdf['text'].str.contains(token_dict[eachkey])].copy()\n",
    "    tmpdf['mutations'] = tmpdf['text'].str.findall(token_dict[eachkey])\n",
    "    tmpmutationslist = tmpdf.explode('mutations').copy()\n",
    "    mutationslist = pd.concat((mutationslist,tmpmutationslist),ignore_index=True)\n",
    "mutationslist['date'] = pd.to_datetime(mutationslist['date'])\n",
    "mutationslist.drop_duplicates(keep='first',inplace=True)\n",
    "mutationslist['gene_mentions'] = mutationslist['text'].str.findall(geneprots)\n",
    "mutationslist['gene_mentions'] = mutationslist['gene_mentions'].apply(lambda x: lowerlist(x))\n",
    "print(mutationslist.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutationsclean = mutationslist[['_id','name','date','mutations','gene_mentions']].copy()\n",
    "mutationsclean.to_csv(os.path.join(RESULTSPATH,'mutations.tsv'),sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "humefactors = mutationslist.loc[mutationslist['text'].str.contains(\"polymorphism\")].copy()\n",
    "humefactors.drop_duplicates(keep=\"first\",inplace=True)\n",
    "humefactors.drop(columns=['abstract','text','description'],inplace=True)\n",
    "humefactors.to_csv(os.path.join(RESULTSPATH,'polymorphisms.tsv'),sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(textdf))\n",
    "#mutdf = textdf.loc[(textdf['text'].str.contains(token_dict['mutants']))].copy()\n",
    "#mutdf['mutations'] = mutdf['text'].str.findall(token_dict['mutants'])\n",
    "\n",
    "#mutdf['date'] = pd.to_datetime(mutdf['date'])\n",
    "#mutationslist = mutdf.explode('mutations').copy()\n",
    "#mutationslist.drop(['abstract','name','description'],axis=1,inplace=True)\n",
    "#sortedmutations = mutationslist.sort_values(['mutations','date'],ascending=[True,True])\n",
    "#mutationfrequency = mutationslist.groupby('mutations').size().reset_index(name='counts')\n",
    "mutationfrequency = mutationslist.groupby('mutations').resample('W-Mon', on='date').size().reset_index(name='counts').sort_values(by='date')\n",
    "mutationfrequency.sort_values(['date','counts'],ascending=[False,False],inplace=True)\n",
    "print(mutationslist.head(n=5))\n",
    "print(mutationfrequency.head(n=20))\n",
    "#singlementions = mutationfrequency['mutations'].loc[mutationfrequency['counts']<2].unique().tolist()\n",
    "#mutations2check = mutationslist.loc[mutationslist['mutations'].isin(singlementions)]\n",
    "#mutations2check.to_csv('data/variants/mutations_to_check.tsv',sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#genemutdf = textdf.loc[textdf['text'].str.contains(token_dict['genemute'],case=False)].copy()\n",
    "#genemutdf = textdf.loc[textdf['text'].str.extract(token_dict['genemute'])].copy()\n",
    "genemutdf = textdf.loc[textdf['text'].str.contains('E484K')].copy()\n",
    "print(genemutdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Variant training set creation\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Variant of Concern 202012/01\", \"VOC-202012/01\", \"20B/501Y.V1\", \"20I/501Y.V1\", the 501Y.V2 variant\n",
    "Variant Under Investigation 202012/01 (VUI 202012/01 for short)\n",
    "\"the {location} variant\"\n",
    "\"variant in {location}\"\n",
    "\"\"\"\n",
    "\n",
    "the_variant = r\"((?i)the (?:\\w*|\\w*.\\w*) variant(?-i))\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Lineage training set creation\n",
    "## More lineage names: \"https://www.the-scientist.com/news-opinion/a-guide-to-emerging-sars-cov-2-variants-68387\"\n",
    "\n",
    "## fetch lineages from :\"https://cov-lineages.org/lineages.html\"\n",
    "lineagetable = read_csv(\"https://raw.githubusercontent.com/cov-lineages/pango-designation/master/lineages.csv\",error_bad_lines=False,header=0)\n",
    "lineages = lineagetable['lineage'].loc[lineagetable['lineage'].str.len()>2].unique().tolist()\n",
    "print(lineagetable,len(lineages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikivariants = variant_names(querylist)\n",
    "wikidict = {}\n",
    "i=0\n",
    "while i < len(wikivariants):\n",
    "    wikidict[wikivariants.iloc[i]['alias'].lower().strip()] = wikivariants.iloc[i]['name'].lower().strip()\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lineage search, no additional regex formatting\n",
    "The issue with this method is that the (.) are not handled properly so things like N95 end up being included and an additional filtering is needed which may end up cutting out relevant entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterlist = list(set(lineages).union(set(wikivariants['alias'].tolist())))\n",
    "searchterm = ' | '.join(masterlist)\n",
    "filterterms = '|'.join(filter_terms)\n",
    "tmpdf = textdf.loc[textdf['text'].str.contains(searchterm)].copy()\n",
    "tmpdf['lineages'] = tmpdf['text'].str.findall(searchterm)\n",
    "rawlineageslist = tmpdf.explode('lineages').copy()\n",
    "cleanlineageslist = rawlineageslist.loc[rawlineageslist['text'].str.contains(filterterms)].copy()\n",
    "cleanlineageslist['lineages'] = [x.strip() for x in cleanlineageslist['lineages']]\n",
    "\n",
    "print(len(rawlineageslist))\n",
    "print(len(cleanlineageslist))\n",
    "    \n",
    "cleanlineageslist['lineages'].replace(wikidict,inplace=True)\n",
    "cleanlineageslist.drop_duplicates(keep='first',inplace=True)\n",
    "print(len(cleanlineageslist))\n",
    "print(cleanlineageslist.head(n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineagecounts = cleanlineageslist.groupby('lineages').size().reset_index(name='publication counts')\n",
    "lineagecounts.sort_values('publication counts',ascending=False,inplace=True)\n",
    "print(lineagecounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional regex attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterlist = list(set(lineages).union(set(wikivariants['alias'].tolist())))\n",
    "#print(len(masterlist))\n",
    "searchterm = ' | '.join(masterlist)\n",
    "#re_str1 = r'\\b(?i)('\n",
    "#re_str2 = r')(?-i)'\n",
    "#rawstring = r\"{}\".format(searchterm)\n",
    "#searchregex = re.compile(re_str1 + rawstring + re_str2)\n",
    "filterterms = '|'.join(filter_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterlist = list(set(lineages).union(set(wikivariants['alias'].tolist())))\n",
    "regexlist = []\n",
    "for eachitem in masterlist:\n",
    "    #rawstring = r\"{}\".format(eachitem.strip().replace('.','\\.'))\n",
    "    searchstring = rf\"{re.escape(eachitem)}\"\n",
    "    #searchregex = re.compile(searchstring)\n",
    "    #print(searchregex)\n",
    "    regexlist.append(searchstring)\n",
    "    #checkdf = textdf.loc[textdf['text'].str.contains(searchregex)].copy()\n",
    "    #if len(checkdf)>0:\n",
    "    #    checkdf['lineages'] = checkdf['text'].str.findall(searchregex)\n",
    "    #    print(checkdf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#searchterms = '|'.join(regexlist)\n",
    "searchregex = re.compile('|'.join(regexlist), re.IGNORECASE)\n",
    "lineagedf = textdf.loc[textdf['text'].str.contains(searchregex)].copy()\n",
    "lineagedf['lineages'] = lineagedf['text'].str.findall(searchregex)\n",
    "print(len(lineagedf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawlineageslist = lineagedf.explode('lineages').copy()\n",
    "#cleanlineageslist = rawlineageslist.loc[rawlineageslist['text'].str.contains(filterterms)].copy()\n",
    "cleanlineageslist = rawlineageslist[['_id','name','date','lineages']].copy()\n",
    "cleanlineageslist['lineages'] = [x.strip().lower() for x in cleanlineageslist['lineages']]\n",
    "\n",
    "print(len(rawlineageslist))\n",
    "print(len(cleanlineageslist))\n",
    "    \n",
    "cleanlineageslist['lineages'].replace(wikidict,inplace=True)\n",
    "cleanlineageslist.drop_duplicates(keep='first',inplace=True)\n",
    "print(len(cleanlineageslist))\n",
    "print(cleanlineageslist.head(n=10))\n",
    "cleanlineageslist.to_csv('results/lineages.tsv',sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = cleanlineageslist.groupby('lineages').size().reset_index(name='counts')\n",
    "frequency.sort_values('counts',ascending=False,inplace=True)\n",
    "print(frequency.head(n=10))\n",
    "print(frequency.tail(n=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check use of elasticsearch to correctly query for ambiguous lineages\n",
    "A development API has been created to test the effect of ignoring '.' in the indexing of name, abstract, keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_dev_size(source):\n",
    "    pubmeta = requests.get(\"https://dev.outbreak.info/resources/query?q=((@type:Publication) AND (curatedBy.name:\"+source+\"))&size=0&aggs=@type\")\n",
    "    pubjson = json.loads(pubmeta.text)\n",
    "    pubcount = int(pubjson[\"facets\"][\"@type\"][\"total\"])\n",
    "    return(pubcount)\n",
    "\n",
    "\n",
    "#### Ping the API and get all the ids for a specific source and scroll through the source until number of ids matches meta\n",
    "def get_dev_ids(source):\n",
    "    source_size = fetch_src_size(source)\n",
    "    r = requests.get(\"https://dev.outbreak.info/resources/query?q=((@type:Publication) AND (curatedBy.name:\"+source+\"))&fields=_id&fetch_all=true\")\n",
    "    response = json.loads(r.text)\n",
    "    idlist = get_ids_from_json(response)\n",
    "    try:\n",
    "        scroll_id = response[\"_scroll_id\"]\n",
    "        while len(idlist) < source_size:\n",
    "            r2 = requests.get(\"https://dev.outbreak.info/resources/query?q=((@type:Publication) AND (curatedBy.name:\"+source+\"))&fields=_id&fetch_all=true&scroll_id=\"+scroll_id)\n",
    "            response2 = json.loads(r2.text)\n",
    "            idlist2 = set(get_ids_from_json(response2))\n",
    "            tmpset = set(idlist)\n",
    "            idlist = tmpset.union(idlist2)\n",
    "            try:\n",
    "                scroll_id = response2[\"_scroll_id\"]\n",
    "            except:\n",
    "                print(\"no new scroll id\")\n",
    "        return(idlist)\n",
    "    except:\n",
    "        return(idlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Extract mutations\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from datetime import datetime\n",
    "from src.common import *\n",
    "from src.extract_variants import *\n",
    "\n",
    "DATAPATH = 'data/'\n",
    "RESULTSPATH = 'results/'\n",
    "\n",
    "\n",
    "allids = get_pub_ids('all')\n",
    "metadf = batch_fetch_meta(allids)\n",
    "textdf = merge_texts(metadf)\n",
    "mutationsclean = extract_mutations(RESULTSPATH,textdf, token_dict, export=True)\n",
    "\n",
    "## Run time of entire script: 31 min\n",
    "## Run time of just the mutation extraction: 24.7 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\outbreak\\COVID19\\variant_extraction\\src\\extract_variants.py\u001b[0m in \u001b[0;36mextract_lineages\u001b[1;34m(RESULTSPATH, textdf, lineagequerylist, export)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mextract_lineages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRESULTSPATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtextdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlineagequerylist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexport\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[0mlineages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_pango_lineages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m     \u001b[0mwikidict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_wiki_variants\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlineagequerylist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m     \u001b[0mmasterlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlineages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwikivariants\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alias'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[0mregexlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\outbreak\\COVID19\\variant_extraction\\src\\extract_variants.py\u001b[0m in \u001b[0;36mget_wiki_variants\u001b[1;34m(lineagequerylist)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_wiki_variants\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlineagequerylist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m     \u001b[0mwikivariants\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvariant_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlineagequerylist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m     \u001b[0mwikidict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\outbreak\\COVID19\\variant_extraction\\src\\extract_variants.py\u001b[0m in \u001b[0;36mvariant_names\u001b[1;34m(lineagequerylist)\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'format'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'json'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'query'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'headers'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'results'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'bindings'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                 variants.append(OrderedDict({\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Extract variants\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from datetime import datetime\n",
    "from src.common import *\n",
    "from src.extract_variants import *\n",
    "\n",
    "DATAPATH = 'data/'\n",
    "RESULTSPATH = 'results/'\n",
    "\n",
    "#allids = get_pub_ids('all')\n",
    "#metadf = batch_fetch_meta(allids)\n",
    "#textdf = merge_texts(metadf)\n",
    "cleanlineageslist = extract_lineages(RESULTSPATH, lineagequerylist, textdf, export=True)\n",
    "\n",
    "## Runtime of just the lineage extraction: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          name                      alias\n",
      "0     SARS-CoV-2 Alpha variant   SARS-CoV-2 Alpha variant\n",
      "1     SARS-CoV-2 Alpha variant                    B.1.1.7\n",
      "2     SARS-CoV-2 Alpha variant            B.1.1.7 lineage\n",
      "3     SARS-CoV-2 Alpha variant                       B117\n",
      "4     SARS-CoV-2 Alpha variant            British variant\n",
      "..                         ...                        ...\n",
      "128          Lineage B.1.617.3          Lineage B.1.617.3\n",
      "130  SARS-CoV-2 Lambda variant  SARS-CoV-2 Lambda variant\n",
      "131  SARS-CoV-2 Lambda variant                     Lambda\n",
      "132  SARS-CoV-2 Lambda variant               lineage C.37\n",
      "133  SARS-CoV-2 Lambda variant                       C.37\n",
      "\n",
      "[130 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def make_request(params):\n",
    "    wikidata_url = 'https://query.wikidata.org/sparql'\n",
    "    r = requests.get(wikidata_url, params)\n",
    "    if r.status_code == 200:\n",
    "        return r\n",
    "    if r.status_code == 500:\n",
    "        return 0\n",
    "    if r.status_code == 403:\n",
    "        return 0\n",
    "    if r.status_code == 429:\n",
    "        timeout = get_delay(r.headers['retry-after'])\n",
    "        print('Timeout {} m {} s'.format(timeout // 60, timeout % 60))\n",
    "        time.sleep(timeout)\n",
    "        make_request(params)\n",
    "\n",
    "url = 'https://query.wikidata.org/sparql'\n",
    "headers = {'User-Agent': 'outbreak variant extraction bot (https://outbreak.info/; help@outbreak.info)'}\n",
    "variants = []\n",
    "for query in lineagequerylist:\n",
    "    params = {'format': 'json', 'query': query, 'headers':headers}\n",
    "    r = make_request(params)\n",
    "    if r == 0:\n",
    "        data = r.json()\n",
    "    else:\n",
    "        break\n",
    "    for item in data['results']['bindings']:\n",
    "        try:\n",
    "            variants.append(OrderedDict({\n",
    "            'name': item['itemLabel']['value'],\n",
    "            'alias': item['itemLabel']['value']}))\n",
    "            tmp= item['itemAltLabel']['value'].split(',')\n",
    "            for altname in tmp:\n",
    "                if len(altname.strip())>3:\n",
    "                    variants.append(OrderedDict({\n",
    "                    'name': item['itemLabel']['value'],\n",
    "                    'alias': altname\n",
    "                    }))\n",
    "        except:\n",
    "            variants.append(OrderedDict({\n",
    "            'name': item['itemLabel']['value'].strip(),\n",
    "            'alias': item['itemLabel']['value'].strip()\n",
    "            }))\n",
    "    time.sleep(1)\n",
    "wikivariants = pd.DataFrame(variants)\n",
    "wikivariants.drop_duplicates(keep='first',inplace=True)\n",
    "print(wikivariants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
